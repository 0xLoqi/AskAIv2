Project Overview & Goals
Skai  is a Windows-based AI assistant that combines state-of-the-art language, vision, and voice AI to provide seamless help to users anywhere on their desktop. The goal is to create a polished, unobtrusive assistant that can answer questions, perform tasks on behalf of the user, and proactively offer helpful suggestions – all triggered by a simple hotkey. Unlike simple screenshot Q&A tools or browser extensions, Skai aims to integrate deeply with the user’s workflow (text, voice, and UI automation) while maintaining a light footprint and respecting user focus. It should exceed the polish and capabilities of existing tools like ShotSolve and Skai, delivering a slick UI/UX and powerful context awareness.
Key Features & Requirements
Universal Hotkey Activation: The assistant is invoked with Ctrl + Shift + Space. A quick press opens the chat interface for text input, whereas holding the hotkey >0.5s initiates voice capture mode.
Sleek Contextual UI: A small chat bubble appears near the cursor when activated, with a semi-transparent modern design. It can be dragged or “pinned” to stay on screen. Clicking outside dismisses it (unless pinned) to avoid intrusions.
System Tray Integration: A tray icon (using a Skai mascot icon) runs persistently. Clicking it opens a larger chat history window, showing past conversations and offering a “New Chat” reset. This tray menu also houses settings (e.g. enabling/disabling vision or voice features).
GPT-4 with Vision Support: The assistant can utilize GPT-4 Vision capabilities to understand the user’s screen. When enabled, it will capture screenshots of the relevant window or area, downscale them for efficiency, and include them in queries so GPT-4 can “see” context. This allows asking questions about on-screen content (images, PDF open, error dialogs, etc.) and getting answers or explanations​
venturebeat.com
.
Voice Input & Output: High-accuracy speech-to-text (using OpenAI’s Whisper or similar) processes the user’s voice queries when the hotkey is held. This transcribed text is fed into the AI. (Initially, responses will be text only on screen; text-to-speech for answers can be a future enhancement.) The voice model should handle natural dictation robustly (for example, using Whisper large-v2 which is “priced at $0.006 / minute” via API​
openai.com
 or an equivalent offline model for privacy).
Context-Aware Nudges: When enabled, Skai will periodically (e.g. every N seconds or on certain events) take a lightweight snapshot of the user’s current screen or active window text and analyze it. Using OCR, it can extract on-screen text and, if a helpful action is apparent, surface a gentle suggestion. For example, if it detects the user writing an email about scheduling, it might suggest “Would you like me to draft a polite meeting invitation?” These nudges use local OCR and heuristics to minimize distraction and cost, only calling GPT for a suggestion when high confidence it would be useful. (The user can adjust or turn off these proactive tips.)
Document & UI Understanding (OmniParser): The assistant leverages Microsoft’s OmniParser to convert screenshots of documents or application UIs into structured data. OmniParser can identify interactive UI elements and extract text, providing a list of elements (buttons, input fields, labels, etc.) with their roles​
huggingface.co
. This forms a foundation for the assistant’s agent mode, enabling it to understand on-screen forms or dialogs in detail.
Agent Capabilities (Task Automation): Beyond Q&A, Skai can act on the user’s behalf in certain tasks. It will use parsed UI structure plus instructions from GPT to perform actions like clicking buttons, typing into fields, or navigating a webpage. Under the hood this uses Selenium WebDriver (for web applications) to programmatically control a browser or web page DOM​
en.wikipedia.org
. For example, if a user says “Log me into my email and draft a message to Alice,” Skai could open the browser (or detect one already open), navigate to the mail site, and fill in the form. All such actions will require user confirmation before execution for safety (following OpenAI’s guideline: “We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users.”​
venturebeat.com
).
Language Support: English at launch (all interface and processing in English). The architecture should be extensible to other languages later, but initial focus is English to simplify voice and LLM handling.
Polish & Usability: In all interactions, Skai (the mascot/assistant persona) will maintain a friendly, helpful tone. The interface should be responsive and fast, avoiding heavy resource usage. The design should inspire confidence (professional look, not cartoonish, but with a touch of personality via the mascot). Compared to existing tools, Skai will feel more integrated and intelligent, not just a proxy to GPT.