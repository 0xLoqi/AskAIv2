AI Processing Layer (LLM & Vision)
This layer handles calls to AI models for both text and image understanding:
OpenAI GPT-4 (Language & Vision): Skai uses OpenAIâ€™s GPT-4 API as the core brain for understanding and responding. GPT-4 is chosen for its superior reasoning and ability to process images (Vision) compared to smaller models. As of 2024, OpenAI has made GPT-4 with Vision generally available via APIâ€‹
venturebeat.com
, allowing a single API call to include both text and an image. We will leverage this by sending the conversation history (as text) along with any relevant screenshot (as binary/image data) to the API. GPT-4 then returns a response that incorporates both modalities. This unified approach is efficient since â€œpreviously, developers had to use separate models for text and images, but now, with just one API call, the model can analyze images and apply reasoning.â€â€‹
venturebeat.com
Prompt Management: The client will construct a prompt with a system message defining Skaiâ€™s persona and capabilities (e.g. a system prompt: â€œYou are Skai, a helpful AI assistant on Windows...â€ to set tone), followed by the conversation (user messages and assistant replies), plus an <image> if applicable. We will use the GPT-4 8k or 16k context variant, which is sufficient for typical use; if needed, we could later upgrade to 32k. The Vision input (screenshot) will usually be accompanied by a short text describing what the user wants from the image (â€œThe userâ€™s screen is shown above. They ask: <user question>â€). This way GPT-4 can ground its answer on the image.
Downscaling Images: To reduce cost and latency, screenshots will be automatically downscaled before sending. For example, a full HD screenshot might be scaled to ~30% size or a max resolution (e.g. 1024px width), balancing legibility of text in the image with fewer pixels to transmit. This significantly cuts down on data sent to GPT-4 Vision (which may count as tokens or affect processing cost). The downscale algorithm can be something like: capture at full res, then resize using a high-quality filter.
OCR fallback: In cases where GPT-4 Vision might be too costly to call frequently (such as for the proactive suggestions), the app can use local OCR to extract text and feed that text into GPT-4 (text-only) instead. The Windows 10/11 OS has a built-in OCR engine available via the Windows.Media.Ocr API, which â€œallows application developers to extract text in a specific language from an image.â€â€‹
learn.microsoft.com
 This is useful to cheaply get screen text and perhaps send a summarized version to GPT-4 (or even a cheaper GPT-3.5 model) to generate a suggestion.
Use of GPT-3.5 (Optional): For less demanding tasks or to save cost, we can utilize GPT-3.5 Turbo for certain queries (especially if no image involved). GPT-3.5 is faster and much cheaper ($0.002/1K tokens vs GPT-4â€™s $0.03-0.06/1Kâ€‹
help.openai.com
). The system could automatically decide: e.g. for very simple Q&A or if the user is on a free tier, use GPT-3.5; for complex or vision queries, use GPT-4. This would be transparent to the user except perhaps a slight difference in answer quality. In early MVP, however, we might stick to GPT-4 for consistency and only add such optimizations if needed.
Whisper Speech-to-Text: For voice input, Skai will utilize Whisper, which is OpenAIâ€™s advanced speech recognition model. Two implementations are possible:
Cloud STT (Whisper API): Easiest integration â€“ send the recorded audio to OpenAIâ€™s Whisper API and get text back. Whisperâ€™s large-v2 model via API is priced at $0.006/min of audioâ€‹
openai.com
, which is very affordable. The app would record audio (in chunks or the whole utterance), then upload it to the API. This yields accurate transcripts with punctuation. The turnaround is fairly quick for short utterances (a few seconds).
Local STT (Whisper offline): For privacy and offline capability, we can ship a local Whisper model. Running Whisper large in real-time on a typical PC CPU is challenging, but Whisper tiny/base models or optimized ports (like whisper.cpp or OpenAIâ€™s Whisper in ONNX) can achieve near real-time transcription. Another possibility is using a smaller â€œSuperWhisperâ€ model â€“ if available â€“ or any faster-than-real-time ASR for responsiveness. The MVP may start with the cloud API (for simplicity and quality) and later offer an â€œoffline modeâ€ setting using a local model for users who prefer no voice data leaves their machine.
Voice Processing Flow: When the hotkey is held, the app continuously listens. We can implement a short voice activity detection to stop on silence or just require the user to release the hotkey when done speaking (similar to a walkie-talkie push-to-talk). Once audio capture stops, the audio is either sent to the Whisper API or processed by local Whisper. The resulting text is inserted into the chat as the userâ€™s query (and also shown in the UI so the user can confirm it heard them correctly). From there, the LLM answers as usual. This makes voice queries as seamless as typing.
Response Handling: The AIâ€™s answer, once received (as text), is displayed in the chat UI. If the answer includes any action instructions (for the agent) or image references (like describing the image), the client will parse that. For example, if using OpenAIâ€™s new function calling abilities, GPT-4 might return a JSON with a desired action (click coordinates or a UI element ID). The client would then prompt the user â€œSkai can perform X for you â€“ proceed?â€ and on confirmation, invoke the automation routines. The app will also handle streaming of responses if supported (showing the answer text as itâ€™s being generated for a faster feel).


Example: The image above shows a similar assistant (ShotSolve on macOS) analyzing a screenshot to provide feedback on a webpage design. Skai will integrate GPT-4 Vision in a Windows environment to achieve such context-aware help. The UI presents the screenshot alongside the query (â€œGive me feedback to improve this landing pageâ€), and GPT-4 Vision responds with suggestions in a chat format. Skaiâ€™s chat overlay will likewise allow users to ask questions about whatâ€™s on their screen and get instant insights or answers grounded in the visual context.
Vision Context and OmniParser (Screen Parsing)
A key innovation for Skai is its ability to not just see pixels, but understand the structure of whatâ€™s on screen. This is where Microsoftâ€™s OmniParser comes into play: 

Above: OmniParser can interpret a UI screenshot and output the textual content and identifiable interface elements. For example, in the mobile UI screenshots, it extracted visible text like â€œTwitterâ€, â€œJournalâ€ and recognized icons (e.g., an icon that looks like â€œa phone call or messaging applicationâ€)â€‹
huggingface.co
. This structured understanding lets an AI agent uniquely refer to parts of the UI (like â€œIcon ID 27â€ for the messaging app icon) and know their function.
How OmniParser is Used: When Skai is in agent mode (i.e., the user expects it to perform an action on the UI), the application will:
Capture a screenshot of the relevant application window (or the full desktop if needed).
Run the OmniParser model on this image locally. OmniParser actually consists of two ML models â€“ a YOLOv8-based detector for interactive regions (buttons, icons, fields) and a BLIP-2-based captioner for describing icon semanticsâ€‹
huggingface.co
. The output is a list of elements with coordinates, element type (button, toggle, link, etc.), any text on them, and descriptive labels for icons.
Convert this output into a prompt context for GPT-4. For example: â€œOn screen: [Button#1: â€˜OKâ€™], [TextBox#2: label â€˜Usernameâ€™], [Icon#3: trash can (delete)] â€¦â€. This local semantic data augments what GPT-4 might glean from raw pixels, making it far more reliable in referring to specific UI elements. (OmniParser significantly improves an LLM agentâ€™s success in UI tasksâ€‹
microsoft.github.io
.)
Ask GPT-4 (with this context plus the userâ€™s request) to determine the actions needed. We can utilize OpenAIâ€™s function calling feature to have GPT return a structured plan, e.g. {"click": 1} meaning â€œclick Button#1 (OK)â€ or {"type": {"field": 2, "text": "Alice"}} meaning â€œtype 'Alice' into TextBox#2â€.
Action Execution (Selenium & OS Automation): Once the plan is obtained, the Skai client carries it out:
For web actions: Selenium WebDriver will be used to drive a browser. Selenium is a popular framework that â€œenables web browser automationâ€â€‹
en.wikipedia.org
 â€“ essentially controlling a browser like a user would. We can use the Chrome or Edge WebDriver to interact with web pages. If the target UI is a web app, OmniParserâ€™s elements can often be mapped to DOM elements (e.g., by text or index), or we might directly instruct Selenium (via element XPaths or CSS selectors, if we have them). For instance, to click a button identified by OmniParser as â€œOKâ€, we find a DOM button with text â€œOKâ€ and call Seleniumâ€™s click.
For desktop app actions: Selenium wonâ€™t work (itâ€™s for browsers). In the future, we might use Windows UI Automation APIs or tools like AutoHotkey/PowerShell scripts to click native app coordinates. However, initial agent focus will be on web (since Selenium excels there and many tasks â€“ email, form filling, web queries â€“ are browser-based).
Safety: Before performing any action, the assistant will always ask user confirmation (e.g., â€œğŸ¤– Skai: Should I click â€˜OKâ€™ on the installer dialog?â€). This way, users stay in control of any automation.
Example Flow: Suppose a user opens an online form and says â€œSkai, fill this form with my info.â€ Skai would take a screenshot of the form, use OmniParser to extract that there are fields like Name, Email, Address. It might then prompt GPT-4 with â€œUser wants to fill the form with their info. Screen elements: {Field1: Name, Field2: Email, â€¦}. The userâ€™s name is John Doe, email john@example.com (if known from user profile).â€ GPT-4 then returns an action plan to input â€œJohn Doeâ€ in Field1, â€œjohn@example.comâ€ in Field2, etc. Skai executes these via Selenium (if itâ€™s a web form) by locating input elements and sending keystrokes. In seconds, the form is filled â€“ saving the user time.
Overall, through OmniParser and automation, Skai moves beyond just answering questions to taking actions. This turns it into a quasi-RPA (robotic process automation) assistant for the userâ€™s personal tasks.