AI Processing Layer (LLM & Vision)
This layer handles calls to AI models for both text and image understanding:
OpenAI GPT-4 (Language & Vision): Ask.AI uses OpenAI‚Äôs GPT-4 API as the core brain for understanding and responding. GPT-4 is chosen for its superior reasoning and ability to process images (Vision) compared to smaller models. As of 2024, OpenAI has made GPT-4 with Vision generally available via API‚Äã
venturebeat.com
, allowing a single API call to include both text and an image. We will leverage this by sending the conversation history (as text) along with any relevant screenshot (as binary/image data) to the API. GPT-4 then returns a response that incorporates both modalities. This unified approach is efficient since ‚Äúpreviously, developers had to use separate models for text and images, but now, with just one API call, the model can analyze images and apply reasoning.‚Äù‚Äã
venturebeat.com
Prompt Management: The client will construct a prompt with a system message defining Skai‚Äôs persona and capabilities (e.g. a system prompt: ‚ÄúYou are Skai, a helpful AI assistant on Windows...‚Äù to set tone), followed by the conversation (user messages and assistant replies), plus an <image> if applicable. We will use the GPT-4 8k or 16k context variant, which is sufficient for typical use; if needed, we could later upgrade to 32k. The Vision input (screenshot) will usually be accompanied by a short text describing what the user wants from the image (‚ÄúThe user‚Äôs screen is shown above. They ask: <user question>‚Äù). This way GPT-4 can ground its answer on the image.
Downscaling Images: To reduce cost and latency, screenshots will be automatically downscaled before sending. For example, a full HD screenshot might be scaled to ~30% size or a max resolution (e.g. 1024px width), balancing legibility of text in the image with fewer pixels to transmit. This significantly cuts down on data sent to GPT-4 Vision (which may count as tokens or affect processing cost). The downscale algorithm can be something like: capture at full res, then resize using a high-quality filter.
OCR fallback: In cases where GPT-4 Vision might be too costly to call frequently (such as for the proactive suggestions), the app can use local OCR to extract text and feed that text into GPT-4 (text-only) instead. The Windows 10/11 OS has a built-in OCR engine available via the Windows.Media.Ocr API, which ‚Äúallows application developers to extract text in a specific language from an image.‚Äù‚Äã
learn.microsoft.com
 This is useful to cheaply get screen text and perhaps send a summarized version to GPT-4 (or even a cheaper GPT-3.5 model) to generate a suggestion.
Use of GPT-3.5 (Optional): For less demanding tasks or to save cost, we can utilize GPT-3.5 Turbo for certain queries (especially if no image involved). GPT-3.5 is faster and much cheaper ($0.002/1K tokens vs GPT-4‚Äôs $0.03-0.06/1K‚Äã
help.openai.com
). The system could automatically decide: e.g. for very simple Q&A or if the user is on a free tier, use GPT-3.5; for complex or vision queries, use GPT-4. This would be transparent to the user except perhaps a slight difference in answer quality. In early MVP, however, we might stick to GPT-4 for consistency and only add such optimizations if needed.
Whisper Speech-to-Text: For voice input, Ask.AI will utilize Whisper, which is OpenAI‚Äôs advanced speech recognition model. Two implementations are possible:
Cloud STT (Whisper API): Easiest integration ‚Äì send the recorded audio to OpenAI‚Äôs Whisper API and get text back. Whisper‚Äôs large-v2 model via API is priced at $0.006/min of audio‚Äã
openai.com
, which is very affordable. The app would record audio (in chunks or the whole utterance), then upload it to the API. This yields accurate transcripts with punctuation. The turnaround is fairly quick for short utterances (a few seconds).
Local STT (Whisper offline): For privacy and offline capability, we can ship a local Whisper model. Running Whisper large in real-time on a typical PC CPU is challenging, but Whisper tiny/base models or optimized ports (like whisper.cpp or OpenAI‚Äôs Whisper in ONNX) can achieve near real-time transcription. Another possibility is using a smaller ‚ÄúSuperWhisper‚Äù model ‚Äì if available ‚Äì or any faster-than-real-time ASR for responsiveness. The MVP may start with the cloud API (for simplicity and quality) and later offer an ‚Äúoffline mode‚Äù setting using a local model for users who prefer no voice data leaves their machine.
Voice Processing Flow: When the hotkey is held, the app continuously listens. We can implement a short voice activity detection to stop on silence or just require the user to release the hotkey when done speaking (similar to a walkie-talkie push-to-talk). Once audio capture stops, the audio is either sent to the Whisper API or processed by local Whisper. The resulting text is inserted into the chat as the user‚Äôs query (and also shown in the UI so the user can confirm it heard them correctly). From there, the LLM answers as usual. This makes voice queries as seamless as typing.
Response Handling: The AI‚Äôs answer, once received (as text), is displayed in the chat UI. If the answer includes any action instructions (for the agent) or image references (like describing the image), the client will parse that. For example, if using OpenAI‚Äôs new function calling abilities, GPT-4 might return a JSON with a desired action (click coordinates or a UI element ID). The client would then prompt the user ‚ÄúSkai can perform X for you ‚Äì proceed?‚Äù and on confirmation, invoke the automation routines. The app will also handle streaming of responses if supported (showing the answer text as it‚Äôs being generated for a faster feel).


Example: The image above shows a similar assistant (ShotSolve on macOS) analyzing a screenshot to provide feedback on a webpage design. Ask.AI will integrate GPT-4 Vision in a Windows environment to achieve such context-aware help. The UI presents the screenshot alongside the query (‚ÄúGive me feedback to improve this landing page‚Äù), and GPT-4 Vision responds with suggestions in a chat format. Ask.AI‚Äôs chat overlay will likewise allow users to ask questions about what‚Äôs on their screen and get instant insights or answers grounded in the visual context.
Vision Context and OmniParser (Screen Parsing)
A key innovation for Ask.AI is its ability to not just see pixels, but understand the structure of what‚Äôs on screen. This is where Microsoft‚Äôs OmniParser comes into play: 

Above: OmniParser can interpret a UI screenshot and output the textual content and identifiable interface elements. For example, in the mobile UI screenshots, it extracted visible text like ‚ÄúTwitter‚Äù, ‚ÄúJournal‚Äù and recognized icons (e.g., an icon that looks like ‚Äúa phone call or messaging application‚Äù)‚Äã
huggingface.co
. This structured understanding lets an AI agent uniquely refer to parts of the UI (like ‚ÄúIcon ID 27‚Äù for the messaging app icon) and know their function.
How OmniParser is Used: When Ask.AI is in agent mode (i.e., the user expects it to perform an action on the UI), the application will:
Capture a screenshot of the relevant application window (or the full desktop if needed).
Run the OmniParser model on this image locally. OmniParser actually consists of two ML models ‚Äì a YOLOv8-based detector for interactive regions (buttons, icons, fields) and a BLIP-2-based captioner for describing icon semantics‚Äã
huggingface.co
. The output is a list of elements with coordinates, element type (button, toggle, link, etc.), any text on them, and descriptive labels for icons.
Convert this output into a prompt context for GPT-4. For example: ‚ÄúOn screen: [Button#1: ‚ÄòOK‚Äô], [TextBox#2: label ‚ÄòUsername‚Äô], [Icon#3: trash can (delete)] ‚Ä¶‚Äù. This local semantic data augments what GPT-4 might glean from raw pixels, making it far more reliable in referring to specific UI elements. (OmniParser significantly improves an LLM agent‚Äôs success in UI tasks‚Äã
microsoft.github.io
.)
Ask GPT-4 (with this context plus the user‚Äôs request) to determine the actions needed. We can utilize OpenAI‚Äôs function calling feature to have GPT return a structured plan, e.g. {"click": 1} meaning ‚Äúclick Button#1 (OK)‚Äù or {"type": {"field": 2, "text": "Alice"}} meaning ‚Äútype 'Alice' into TextBox#2‚Äù.
Action Execution (Selenium & OS Automation): Once the plan is obtained, the Ask.AI client carries it out:
For web actions: Selenium WebDriver will be used to drive a browser. Selenium is a popular framework that ‚Äúenables web browser automation‚Äù‚Äã
en.wikipedia.org
 ‚Äì essentially controlling a browser like a user would. We can use the Chrome or Edge WebDriver to interact with web pages. If the target UI is a web app, OmniParser‚Äôs elements can often be mapped to DOM elements (e.g., by text or index), or we might directly instruct Selenium (via element XPaths or CSS selectors, if we have them). For instance, to click a button identified by OmniParser as ‚ÄúOK‚Äù, we find a DOM button with text ‚ÄúOK‚Äù and call Selenium‚Äôs click.
For desktop app actions: Selenium won‚Äôt work (it‚Äôs for browsers). In the future, we might use Windows UI Automation APIs or tools like AutoHotkey/PowerShell scripts to click native app coordinates. However, initial agent focus will be on web (since Selenium excels there and many tasks ‚Äì email, form filling, web queries ‚Äì are browser-based).
Safety: Before performing any action, the assistant will always ask user confirmation (e.g., ‚Äúü§ñ Skai: Should I click ‚ÄòOK‚Äô on the installer dialog?‚Äù). This way, users stay in control of any automation.
Example Flow: Suppose a user opens an online form and says ‚ÄúSkai, fill this form with my info.‚Äù Ask.AI would take a screenshot of the form, use OmniParser to extract that there are fields like Name, Email, Address. It might then prompt GPT-4 with ‚ÄúUser wants to fill the form with their info. Screen elements: {Field1: Name, Field2: Email, ‚Ä¶}. The user‚Äôs name is John Doe, email john@example.com (if known from user profile).‚Äù GPT-4 then returns an action plan to input ‚ÄúJohn Doe‚Äù in Field1, ‚Äújohn@example.com‚Äù in Field2, etc. Ask.AI executes these via Selenium (if it‚Äôs a web form) by locating input elements and sending keystrokes. In seconds, the form is filled ‚Äì saving the user time.
Overall, through OmniParser and automation, Ask.AI moves beyond just answering questions to taking actions. This turns it into a quasi-RPA (robotic process automation) assistant for the user‚Äôs personal tasks.